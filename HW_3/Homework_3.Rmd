---
title: "HW_3"
author: "Hector Santana"
date: "10/21/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(DataExplorer)
library(knitr)
library(psych)
library(caret)
library(gridExtra)
library(geoR)
```

# HOMEWORK 3

Hector Santana, Zachary Safir, Mario Pena
 
In this assignment we will explore, analyze and build a binary logistic regression model to predict wheter a particular neighborhood will be at risk for high crime levels. 

We are provided with information on 466 neighborhoods, 12 predictor variables and 1 response variable. The response variable indicates whether the crime rate is above the median (1) or not (0).

We first download the "training" and "evaluation" datasets from GitHub:

```{r Data Importation}
training <- read.csv("https://raw.githubusercontent.com/manonfire86/Data621/main/HW_3/crime-training-data_modified.csv", header=TRUE, sep=",")

evaluation <- read.csv("https://raw.githubusercontent.com/manonfire86/Data621/main/HW_3/crime-evaluation-data_modified.csv", header=TRUE, sep=",")

training[,c("chas","target")] <- lapply(training[,c("chas","target")],factor)
```


# DATA EXPLORATION

Below we have created a table with the summary statistics of our 12 predictor variables.

```{r}
summary <- describe(training[,c(1:12)])
kable(summary)
```


According to our summary statistics above, and our graph below, there are no missing values in our dataset.

```{r}
missing_values <- training %>%
  mutate(id = row_number()) %>%
  gather(-id, key = "key", value = "val") %>%
  mutate(isna = is.na(val)) %>%
  ggplot(aes(key, id, fill = isna)) +
    geom_raster(alpha=0.8) +
    scale_fill_manual(name = "",
        values = c('steelblue', 'tomato3'),
        labels = c("Present", "Missing")) +
    labs(x = "Variable",
           y = "Row Number", title = "Missing Values in Rows") +
    coord_flip()

missing_values
```


We can also observe that the "target" variable is evenly distributed between the "0" and "1" responses.

```{r}
prop.table(table(training$target))
```


Let's make some obervations based on the histogram and boxplot of the variables in our data.

```{r}
plot_histogram(training)
plot_boxplot(data = training, by = "target")

```


We can also observe the correlation of our variables with eachother on the next plot.

```{r}
plot_correlation(training)
```

It seems that "target" has a strong correlation with the variables "nox", "rad", "age", "tax" and "indus".


# DATA PREPARATION

Our data looks relatively clean and it doesn't seem there is much need to do any modifications. However, we do have predictor variables that are skewed or do not follow a normal distribution. We can make use of a Box cox function in order to figure out what is the best transformation that can be applied to these variables in order to normalize them.

A few of the variables that seem skewed or don't follow a normal distribution include: "age", "dis", "indus", "lstat", "nox", "ptratio", "rad" and "tax". 

We will use the "boxcoxfit" function from the "geoR" package to extract the fitted parameters and use the value of lambda for the transformations of each of the variables mentioned above.

Please note that "ptratio" has a lambda of 4.14, which is larger than the suggested highest value of 2 for transformations, thus we will not be transforming this variable.

```{r}
boxcoxfit(training$age)
boxcoxfit(training$dis)
boxcoxfit(training$indus)
boxcoxfit(training$lstat)
boxcoxfit(training$nox)
boxcoxfit(training$ptratio)
boxcoxfit(training$rad)
boxcoxfit(training$tax)
```

```{r}
age_tr <- ggplot(training, aes(age^1.32)) + geom_histogram(fill = 'gray', binwidth = 50, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of Age Transformed') + theme(plot.title = element_text(hjust = 0.5))


dis_tr <- ggplot(training, aes(dis^-0.15)) + geom_histogram(fill = 'gray', binwidth = 0.05, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of Dis Transformed') + theme(plot.title = element_text(hjust = 0.5))  


indus_tr <- ggplot(training, aes(indus^0.44)) + geom_histogram(fill = 'gray', binwidth = 0.25, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of Indus Transformed') + theme(plot.title = element_text(hjust = 0.5))  


lstat_tr <- ggplot(training, aes(lstat^0.23)) + geom_histogram(fill = 'gray', binwidth = 0.2, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of Lstat Transformed') + theme(plot.title = element_text(hjust = 0.5)) 


nox_tr <- ggplot(training, aes(nox^-0.95)) + geom_histogram(fill = 'gray', binwidth = 0.2, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of Nox Transformed') + theme(plot.title = element_text(hjust = 0.5))


rad_tr <- ggplot(training, aes(rad^-0.16)) + geom_histogram(fill = 'gray', binwidth = 0.1, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of Rad Transformed') + theme(plot.title = element_text(hjust = 0.5))


tax_tr <- ggplot(training, aes(tax^-0.5)) + geom_histogram(fill = 'gray', binwidth = 0.01, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of Tax Transformed') + theme(plot.title = element_text(hjust = 0.5))


grid.arrange(age_tr, dis_tr, indus_tr, lstat_tr, nox_tr, rad_tr, tax_tr, ncol=3, nrow=3)
```

# BUILDING MODELS

Given that the dataset we are working with is fairly small, we will use a K-Fold Cross Validation technique to split our data into an additional training and test set in order to use 80% of it in the models and then evaluate their performance with the predictions against the remaining 20%.

```{r partition}
set.seed(120)
train_index <- createDataPartition(training$target, p=.8, list=FALSE, times = 1)
additional_train <- training[train_index,]
additional_test <- training[-train_index,]
```

Model with all predictor variables:

```{r}
model_1 <- train(target ~., data = additional_train, 
              method = "glm", family = "binomial",
              trControl = trainControl(
                  method = "cv", number = 10,
                  savePredictions = TRUE),
              tuneLength = 5, 
              preProcess = c("center", "scale"))
summary(model_1)
```

Model with high collinearity between response and predictor variables:

```{r}
model_2 <- train(target ~ nox + rad + age + tax + indus, data = additional_train, 
              method = "glm", family = "binomial",
              trControl = trainControl(
                  method = "cv", number = 10,
                  savePredictions = TRUE),
              tuneLength = 5, 
              preProcess = c("center", "scale"))
summary(model_2)
```

Model with transformed variables:

```{r}
model_3 <- train(target ~ zn + I(indus^0.44) + chas + I(nox^-0.95) + rm + I(age^1.32) + I(dis^-0.15) + I(rad^-0.16) + I(tax^-0.5) + ptratio + I(lstat^0.23) + medv, data = additional_train, 
              method = "glm", family = "binomial",
              trControl = trainControl(
                  method = "cv", number = 10,
                  savePredictions = TRUE),
              tuneLength = 5, 
              preProcess = c("center", "scale"))
summary(model_3)
```



# SELECT MODELS
