---
title: "HW_3"
author: "Hector Santana"
date: "10/21/2021"
output: pdf_document
---

# Library Loading

```{r warning=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(tidyr)
library(data.table)
library(VIM)
library(mice)
library(Rcpp)
library(corrplot)
library(stats)
library(rstatix)
library(DataExplorer)
library(caret)
library(psych)
library(janitor)
library(DMwR)
library(knitr)
library(gridExtra)
library(geoR)
library(knitr)
library(grid)
library(gridExtra)
library(lmtest)
library(pscl)
library(MKmisc)
library(ROCR)
library(survey)
```

# HOMEWORK 3

Hector Santana, Zachary Safir, Mario Pena
 
In this assignment we will explore, analyze and build a binary logistic regression model to predict whether a particular neighborhood will be at risk for high crime levels. 

We are provided with information on 466 neighborhoods, 12 predictor variables and 1 response variable. The response variable indicates whether the crime rate is above the median (1) or not (0).

We first download the "training" and "evaluation" datasets from GitHub:

# Data Exploration

```{r Data_Importation}

eval_data = read.csv("https://raw.githubusercontent.com/manonfire86/Data621/main/HW_3/crime-evaluation-data_modified.csv")

training_data = read.csv("https://raw.githubusercontent.com/manonfire86/Data621/main/HW_3/crime-training-data_modified.csv")

training_data[,c("chas","target")] <- lapply(training_data[,c("chas","target")],factor)

```

```{r Data_Exploration}

## Initial Analysis of data structures

head(eval_data,5)

head(training_data,5)
```


```{r Data_Exploration_2}

## Initial Analysis of data structures (cont'd)

str(eval_data)
str(training_data)

```

```{r Data_Exploration_3}

## Data Set Comparison

list(training_data,eval_data) %>% compare_df_cols() %>% `colnames<-`(c('Column Name', 'Training', 'Evaluation'))
```

Below we have created a table with the summary statistics of our 12 predictor variables.

```{r Data_Exploration_4}
summary <- describe(training_data[,c(1:12)])
kable(summary)
```

We can also observe that the "target" variable is evenly distributed between the "0" and "1" responses.

```{r Data_Exploration_5}
prop.table(table(training_data$target))
```

Let's make some obervations based on the histogram and boxplot of the variables in our data.

```{r Data_Exploration_6}
training_data  %>% plot_histogram()
training_data %>% plot_density()
training_data %>% plot_boxplot(by="target")
```

We have quite a few variables that are skewed or not normally distributed based on our plots above. We will have to do some type of transformation for some of the variables if we would like to include them in our models.

Additionally, according to our summary statistics above, and our graph below, there are no missing values in our dataset.

```{r Data_Exploration_7}
training_data  %>% plot_missing()
training_data  %>% gather(variable, value) %>% filter(value == 0) %>% 
  group_by(variable) %>% tally() %>% mutate(percent = n / nrow(training_data) * 100) %>%
  mutate(percent = paste0(round(percent, ifelse(percent < 10, 1, 0)), "%")) %>%
  arrange(desc(n)) %>% rename(`Variable With Zeros` = variable, 
                              `Number of Records` = n,
                              `Share of Total` = percent)
```

We can also observe the correlation of our variables with eachother on the next plot.

```{r Data_Exploration_8}
plot_correlation(training_data)
```

It seems that "target_1" has a strong correlation with the variables "nox", "rad", "age", "tax" and "indus".
Target_0 has the inverse relationship for these same variables.

# DATA PREPARATION

Our data looks relatively clean and it doesn't seem there is much need to do any modifications. However, we do have predictor variables that are skewed or do not follow a normal distribution. We can make use of a Box cox function in order to figure out what is the best transformation that can be applied to these variables in order to normalize them.

A few of the variables that seem skewed or don't follow a normal distribution include: "age", "dis", "indus", "lstat", "nox", "ptratio", "rad" and "tax". 

We will use the "boxcoxfit" function from the "geoR" package to extract the fitted parameters and use the value of lambda for the transformations of each of the variables mentioned above.

Please note that "ptratio" has a lambda of 4.14, which is outside of the suggested range of -2 to 2 for transformations, thus we will not be transforming this variable.

```{r}
boxcoxfit(training_data$age)
boxcoxfit(training_data$dis)
boxcoxfit(training_data$indus)
boxcoxfit(training_data$lstat)
boxcoxfit(training_data$nox)
boxcoxfit(training_data$ptratio)
boxcoxfit(training_data$rad)
boxcoxfit(training_data$tax)
```

Below, we can observe that even after transforming the variables, some of them do not follow a nearly normal distribution still, but we were at least able to bring them closer to normalization

```{r}
age_tr <- ggplot(training_data, aes(age^1.32)) + geom_histogram(fill = 'gray', binwidth = 50, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of Age Transformed') + theme(plot.title = element_text(hjust = 0.5))
dis_tr <- ggplot(training_data, aes(dis^-0.15)) + geom_histogram(fill = 'gray', binwidth = 0.05, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of Dis Transformed') + theme(plot.title = element_text(hjust = 0.5))  
indus_tr <- ggplot(training_data, aes(indus^0.44)) + geom_histogram(fill = 'gray', binwidth = 0.25, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of Indus Transformed') + theme(plot.title = element_text(hjust = 0.5))  
lstat_tr <- ggplot(training_data, aes(lstat^0.23)) + geom_histogram(fill = 'gray', binwidth = 0.2, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of Lstat Transformed') + theme(plot.title = element_text(hjust = 0.5)) 
nox_tr <- ggplot(training_data, aes(nox^-0.95)) + geom_histogram(fill = 'gray', binwidth = 0.2, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of Nox Transformed') + theme(plot.title = element_text(hjust = 0.5))
rad_tr <- ggplot(training_data, aes(rad^-0.16)) + geom_histogram(fill = 'gray', binwidth = 0.1, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of Rad Transformed') + theme(plot.title = element_text(hjust = 0.5))
tax_tr <- ggplot(training_data, aes(tax^-0.5)) + geom_histogram(fill = 'gray', binwidth = 0.01, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of Tax Transformed') + theme(plot.title = element_text(hjust = 0.5))
grid.arrange(age_tr, dis_tr, indus_tr, lstat_tr, nox_tr, rad_tr, tax_tr, ncol=3, nrow=3)
```

# BUILDING MODELS

Given that the dataset we are working with is fairly small, we will use a K-Fold Cross Validation technique to train the models. Additionally, we will split our data into an additional training and test set in order to use 80% of it in the models and then evaluate their performance with the predictions against the remaining 20%.

```{r partition}
set.seed(120)
train_index <- createDataPartition(training_data$target, p=.8, list=FALSE, times = 1)
additional_train <- training_data[train_index,]
additional_test <- training_data[-train_index,]
```

Our first model includes all predictor variables:

```{r}
model_1 <- train(target ~., data = additional_train, 
              method = "glm", family = "binomial",
              trControl = trainControl(
                  method = "cv", number = 10,
                  savePredictions = TRUE),
              tuneLength = 5, 
              preProcess = c("center", "scale"))
summary(model_1)
```

For our second model, we have chosen the variables with high collinearity between the response and predictor variables:

```{r}
model_2 <- train(target ~ nox + rad + age + tax + indus, data = additional_train, 
              method = "glm", family = "binomial",
              trControl = trainControl(
                  method = "cv", number = 10,
                  savePredictions = TRUE),
              tuneLength = 5, 
              preProcess = c("center", "scale"))
summary(model_2)
```

Lastly, the third model includes all predictor variables but those that do not follow a nearly normal distribution have been transformed:

```{r}
model_3 <- train(target ~ zn + I(indus^0.44) + chas + I(nox^-0.95) + rm + I(age^1.32) + I(dis^-0.15) + I(rad^-0.16) + I(tax^-0.5) + ptratio + I(lstat^0.23) + medv, data = additional_train, 
              method = "glm", family = "binomial",
              trControl = trainControl(
                  method = "cv", number = 10,
                  savePredictions = TRUE),
              tuneLength = 5, 
              preProcess = c("center", "scale"))
summary(model_3)
```



# SELECT MODELS

First we need to measure performance of the models prior to selection.

```{r model_perf_diagnostics}

plot(model_1$finalModel)
plot(model_2$finalModel)
plot(model_3$finalModel)
```


```{r model_pred_results}
results_model_one = predict(model_1,newdata = additional_test)
results_model_two = predict(model_2,newdata = additional_test)
results_model_three = predict(model_3,newdata = additional_test)

results_model_one_prob = predict(model_1,newdata = additional_test,type = 'prob')
results_model_two_prob = predict(model_2,newdata = additional_test,type = 'prob')
results_model_three_prob = predict(model_3,newdata = additional_test,type = 'prob')



confusionMatrix(data=results_model_one, additional_test$target)
confusionMatrix(data=results_model_two, additional_test$target)
confusionMatrix(data=results_model_three, additional_test$target)
```

```{r model_config_perf}
mod_fit1 = glm(target ~., data = additional_train, family = "binomial")
mod_fit2 = glm(target ~ nox + rad + age + tax + indus, data = additional_train, family = "binomial")
mod_fit3 = glm(target ~ zn + I(indus^0.44) + chas + I(nox^-0.95) + rm + I(age^1.32) + I(dis^-0.15) + I(rad^-0.16) + I(tax^-0.5) + ptratio + I(lstat^0.23) + medv, data = additional_train, family = "binomial")
```

```{r model_sel_support}

# wholistic model
varImp(mod_fit1)

# transformed wholistic model
varImp(mod_fit3)

# test lowest two important variables
regTermTest(mod_fit1, "indus")
regTermTest(mod_fit1, "rm")

```

```{r model_perf_analy}

# anova analysis
anova(mod_fit1,mod_fit2,mod_fit3)

# Goodness of fit tests

anova(mod_fit1,mod_fit2,mod_fit3, test ="Chisq")


## Likelihood Ratio

lrtest(mod_fit1,mod_fit2,mod_fit3)

# Psuedo R^2

pR2(mod_fit1)
pR2(mod_fit2)
pR2(mod_fit3)

# Hosmer-Lemeshow Test

HLgof.test(fit = fitted(mod_fit1), obs = additional_train$target)
HLgof.test(fit = fitted(mod_fit2), obs = additional_train$target)
HLgof.test(fit = fitted(mod_fit3), obs = additional_train$target)

prob_one <- predict(mod_fit1, newdata=additional_test, type="response")
pred_one <- prediction(prob_one, additional_test$target)
perf_one <- performance(pred_one, measure = "tpr", x.measure = "fpr")
plot(perf_one)
auc_mod1 <- performance(pred_one, measure = "auc")
auc_mod1 <- auc_mod1@y.values[[1]]
auc_mod1


prob_two <- predict(mod_fit2, newdata=additional_test, type="response")
pred_two <- prediction(prob_two, additional_test$target)
perf_two <- performance(pred_two, measure = "tpr", x.measure = "fpr")
plot(perf_two)
auc_mod2 <- performance(pred_two, measure = "auc")
auc_mod2 <- auc_mod2@y.values[[1]]
auc_mod2

prob_three <- predict(mod_fit3, newdata=additional_test, type="response")
pred_three <- prediction(prob_three, additional_test$target)
perf_three <- performance(pred_three, measure = "tpr", x.measure = "fpr")
plot(perf_three)
auc_mod3 <- performance(pred_three, measure = "auc")
auc_mod3 <- auc_mod3@y.values[[1]]
auc_mod3

```




