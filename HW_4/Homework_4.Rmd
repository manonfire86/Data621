---
title: "Homework 4"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F)
```

```{r}

library(MASS)
library(tidyverse)
library(DataExplorer)
library(knitr)
library(psych)
library(caret)
library(gridExtra)
library(stats)
library(geoR)
library(arsenal)
library(forecast)
library(e1071)
library(car)
```

# HOMEWORK 4

Hector Santana, Zachary Safir, Mario Pena
 
In this assignment we will explore, analyze and build a multiple linear regression and binary logistic model based on auto insurance data to predict the probability that a person will crash their car and the amount of money it would cost in the event of that person crashing the car.

We are provided with information on a little over 8,000 customers at an auto insurance company. Each record has two response variables. TARGET_FLAG has a response of 1 if the customer was involved in a crash, or 0 if the customer was not involved in a crash.TARGET_AMT has a response of 0 if the customer did not crash their car, or a value greater than 0 otherwise. Additionally, there are 23 predictor variables in the data that could be of use for the model.

We first download the "training" and "evaluation" datasets from GitHub:

```{r Data Importation}
training <- read_csv("https://raw.githubusercontent.com/manonfire86/Data621/main/HW_4/insurance_training_data.csv",col_select = !c("INDEX"))

evaluation <- read_csv("https://raw.githubusercontent.com/manonfire86/Data621/main/HW_4/insurance-evaluation-data.csv",col_select = !c("INDEX"))


```


```{r}
training
```


Before we begin the data exploration process we will clean our data a bit in order to run summary statistics and plots accurately and effectively.





```{r}

num_list <- c("in","ho","bl","ol")
fac_list <- c("ds","iv","q","g")



training <- training %>%   mutate(
                      
                      across(starts_with(num_list), ~str_replace_all(.,"[$,]", ""  ) %>% as.numeric),
                      across(where(is.character) | ends_with(fac_list), ~str_replace_all(.,"z_", "")  %>%  as.factor))

evaluation <- evaluation %>%   mutate(
                      
                      across(starts_with(num_list), ~str_replace_all(.,"[$,]", ""  ) %>% as.numeric),
                      across(where(is.character) | ends_with(fac_list), ~str_replace_all(.,"z_", "")  %>%  as.factor))

```









```{r}
plot_boxplot(lm_training,by="SEX")

plot_boxplot(lm_training,by="CAR_USE")
plot_boxplot(glm_training, by="TARGET_FLAG")
```


```{r}
training %>% count(CLM_FREQ)
```




# DATA EXPLORATION

Below we have created a table with the summary statistics of our 23 predictor variables.

```{r, echo=FALSE, warning=FALSE}
summary <- describe(training[,c(3:25)])[,c(2:5,8,9,11,12)]
knitr::kable(summary)


my_controls <- tableby.control(
  numeric.stats = c("meansd", "medianq1q3", "range","Nmiss"), 
  test = F,
  digits=2
)

kable(summary(tableby(TARGET_FLAG~.,training, control = my_controls),text=T),booktabs = TRUE) 


```  

According to our summary statistics above, and our graph below, there are a few columns (variables) that have missing values. These include "AGE", "INCOME", "YOJ", "HOME_VAL", and "CAR_AGE".

```{r}
plot_missing(training)
```

We can also observe that the "TARGET_FAG" variable is not evenly distributed between the "0" and "1" responses. We have more than double the amount of 0 responses throughout the data.

```{r}
prop.table(table(training$TARGET_FLAG))
```

The insight gained from the statistical analysis permitted us to make note of further data of interest that needed to be analyzed in depth prior to the creation of our models. To confirm these irregularities we then constructed visual representations consisting of density plots, histograms, and boxplots.

We can observe from the histograms below that our second response variable "TARGET_AMT" exhibits extreme right skewness.

```{r}
quant_var <- split_columns(training)
plot_histogram(quant_var$continuous)
```

```{r}
plot_bar(quant_var$discrete)
```

```{r}
plot_density(quant_var$continuous)
```

```{r}
plot_boxplot(
  data = training,
  by = "TARGET_FLAG")+ 
  geom_jitter()
```

We can observe above that perhaps the only distribution that seems close to normal is that of the variable "AGE", as it is also evident in the boxplot against the response variable "TARGET_FLAG". The other variable that could potentially be close to normality is "YOJ", but it seems to have a bimodal distribution because of the large number of peopole with years on the job around 0 to 1.

We were also given some theoretical effects (claims) about some of the variables in the data in regards to how they influence the response variable "TARGET_FLAG" and the probability of collision. 

So far looking at the boxplots, we can see that some of the theoretical effects tend to be more true than others, however, we are unable to see the effects of our discrete variables against the responce variable "TARGET_FLAG". Below we have constructed some tables to get a sense of whether the claims about these variables tend to be true or not.

PARENT1 -  Single Parent. Claim: This has an unknown effect

At a glance, we can see that those customers who are single parents have a very high proportion for being in a car crash. However, it is hard to tell if there is a correlation given the majority of the data are from customers that are "Not" single parents.

```{r}
tbl1 <- addmargins(table(PARENT1=training$PARENT1, TARGET_FLAG=training$TARGET_FLAG))
tbl1
round(prop.table(tbl1[1:2,1:2], margin=1),2)
```

MSTATUS - Marital Status.  Claim: In theory, married people drive more safely. 

There seems to be a balanced split between married and not married customers in our data. We can also observe that those who were involved in a car crash are evenly split between the married and not married customers. However, the proportion of those who did not crash their car tends to be higher in the married category.

```{r}
tbl2 <- addmargins(table(MSTATUS=training$MSTATUS, TARGET_FLAG=training$TARGET_FLAG))
tbl2
round(prop.table(tbl2[1:2,1:2], margin=1),2)
```

SEX - Gender. Claim: Urban legend says that women have less crashes than men. Is that true?. 

There seems to be a balanced split between male and female customers in our data. Below we can also observe that the data is evenly split between males and females in regards to crashing or not crashing their cars, suggesting the claim may be flawed.

```{r}
tbl3 <- addmargins(table(SEX=training$SEX, TARGET_FLAG=training$TARGET_FLAG))
tbl3
round(prop.table(tbl3[1:2,1:2], margin=1),2)
```

EDUCATION - Max Education Level. Claim: Unknown effect, but in theory more educated people tend to drive more safely. 

Given that most of the data come from those customers with high school, bachelors and masters education, the proportions also seem to correspond among those who crashed and didn't crash their car. However, there seems to be a pattern for higher proportions of car crashes within the categories with lower education.

```{r}
tbl4 <- (addmargins(table(EDUCATION=training$EDUCATION, TARGET_FLAG=training$TARGET_FLAG)))
tbl4
round(prop.table(tbl4[1:5,1:2], margin=1),2)
```

JOB - Job Category. Claim: In theory, white collar jobs tend to be safer. 

We can see in the table below that blue collar jobs, students and home makers have the highest proportion of customers who have crashed their cars within their category, thus the claim may have some truth to it.

```{r}
tbl5 <- (addmargins(table(JOB=training$JOB, TARGET_FLAG=training$TARGET_FLAG)))
tbl5
round(prop.table(tbl5[1:9,1:2], margin=1),2)
```

CAR_USE - Vehicle Use.  Claim: Commercial vehicles are driven more, so might increase probability of collision. 

About 63% of the car usage is private, but we can see that those customers who have crashed their car has a higher percentage in the category for commercial usage, suggesting that the claim is true about the increased probability of collision for commercial vehicles.

```{r}
tbl6 <- addmargins(table(CAR_USE=training$CAR_USE, TARGET_FLAG=training$TARGET_FLAG))
tbl6
round(prop.table(tbl6[1:2,1:2], margin=1),2)
```

CAR_TYPE - Type of Car. Claim: Unknown effect on probability of collision, but probably affect the payout if there is a crash. 

We can see that even though sports cars is about 11% of the data we have, they have the highest proportion of car crashes within their category. We can also see that SUVs and Pickups are among the categories with the highest proportions of car crashes, while Minivans have the lowest proportion of car crashes in its category.

```{r}
tbl7 <- addmargins(table(CAR_TYPE=training$CAR_TYPE, TARGET_FLAG=training$TARGET_FLAG))
tbl7
round(prop.table(tbl7[1:6,1:2], margin=1),2)
```

RED_CAR -  A Red Car. Claim: Urban legend says that red cars (especially red sports cars) are more risky. Is that true?. 

We can observe below that roughly 25% of cars in each category were involved in a car crash, and this may disprove the claim that red cars are more risky.

```{r}
tbl8 <- addmargins(table(RED_CAR=training$RED_CAR, TARGET_FLAG=training$TARGET_FLAG))
tbl8
round(prop.table(tbl8[1:2,1:2], margin=1),2)
```

REVOKED - License Revoked (Past 7 Years). Claim: If your license was revoked in the past 7 years, you probably are a more risky driver.

Although only 12% of drivers in the training data have a former license suspension on record, their proportion of being involved in a car crash is twice as high as those who didn't, suggesting the claim may be true.

```{r}
tbl9 <- (addmargins(table(REVOKED=training$REVOKED,TARGET_FLAG=training$TARGET_FLAG)))
tbl9
round(prop.table(tbl9[1:2,1:2], margin=1),2)
```

URBANICITY - Home/Work Area. Claim: Unknown

We can see that the category highly urban has a higher proportion of car crashes, but this may be due to the fact that we have a lot more data from this category, roughly 80% comes from it.

```{r}
tbl10 <- (addmargins(table(URBANICITY=training$URBANICITY, TARGET_FLAG=training$TARGET_FLAG)))
tbl10
round(prop.table(tbl10[1:2,1:2], margin=1),2)
```





```{r}
corr_simple <- function(data=df,sig=0.3){
  
  library(corrplot)
  #convert data to numeric in order to run correlations
  #convert to factor first to keep the integrity of the data - each value will become a number rather than turn into NA
  df_cor <- data %>% mutate_if(is.character, as.factor)
  df_cor <- df_cor %>% mutate_if(is.factor, as.numeric)  #run a correlation and drop the insignificant ones
  corr <- cor(df_cor)
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA   #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr)   #select significant values  
  corr <- subset(corr, abs(Freq) > sig) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),]   #print table
  print(corr)  #turn corr back into matrix in order to plot with corrplot
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
  #plot correlations visually
  corrplot(mtx_corr, is.corr=FALSE, tl.col="black", na.label=" ")
}
```




```{r}
corr_simple(training)
```



```{r}
corr_simple(filter(training, TARGET_FLAG==1))



```







```{r}





vif(lm(TARGET_AMT ~., filter(training, TARGET_FLAG==1) %>% select(!TARGET_FLAG) ))



vif(glm(TARGET_FLAG ~. -TARGET_AMT      , training,family = binomial ))
```

# DATA PREPARATION





We seem to have an error in one of the values for "CAR_AGE", which is -3. As we know this must be a mistake, we will turn it into a missing value.

```{r}
training$CAR_AGE[training$CAR_AGE <0] <- NA



```





Since the following variables with missing data ("INCOME", "YOJ", "HOME_VAL", and "CAR_AGE") are showing skewness in their distribution we have decided to use the median as the replacement of the missing values in order to avoid any bias introduced to the mean due to the skewness itself. 

```{r}



training <- training %>% mutate(across(where(is.numeric), ~case_when(is.na(.) ~median.default(.,na.rm = TRUE), T~.  )),
                                 JOB= case_when(
                                    is.na(JOB)~"Unknown",T~ as.character(JOB)) %>% as.factor)
evaluation <- evaluation %>% mutate(across(where(is.numeric), ~case_when(is.na(.) ~median.default(.,na.rm = TRUE), T~.  )),
                                 JOB= case_when(
                                    is.na(JOB)~"Unknown",T~ as.character(JOB)) %>% as.factor)



```








```{r}
lm_training <- training %>% 
                          filter(TARGET_FLAG==1) %>% select(!TARGET_FLAG)


glm_training <-  training %>% 
                    select(!TARGET_AMT)
```








We will next take a look at some of the variables and see what transformations may be used.

INCOME

Income is a right skewed variable with a significant number zeroes. We will apply the square root transformation suggested by the box-cox function to the original variable to reduce the overall skewness.






```{r}


lam_inc <- boxcoxfit((training$INCOME+1))$lambda 



ggplot(training, aes((INCOME+1)^lam_inc)) + geom_histogram(fill = 'gray', binwidth = 10, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of INCOME Transformed') + theme(plot.title = element_text(hjust = 0.5))
```






YOJ

Years on the job seems to have a bimodal distribution with a large number of customers with 0-1 years. We have applied the suggested transformation to the variable to bring it closer to normality.

```{r}
lam_YOJ <- boxcoxfit(training$YOJ+1)$lambda

ggplot(training, aes((YOJ+1)^1.48)) + geom_histogram(fill = 'gray', binwidth = 10, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of YOJ Transformed') + theme(plot.title = element_text(hjust = 0.5))
```

HOME_VAL

Home values are also moderately right skewed with a significant number of zeroes. We have applied the suggested transformation to this variable to reduce the overall skewness but as you can see below, it does not help much because of the significant number of 0 values in our data.

```{r}
lam_home <- boxcoxfit(training$HOME_VAL+1)$lambda

ggplot(training, aes((HOME_VAL+1)^lam_home)) + geom_histogram(fill = 'gray', binwidth = 1, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of HOME_VAL Transformed') + theme(plot.title = element_text(hjust = 0.5))
```

CAR_AGE

The age of the cars follow a bimodal distribution because of the significant number of cars that are close to 0 or 1 year of age. We have applied the suggested transformation, but again as we can see below, it has not helped much.

```{r}
lam_car_a <- boxcoxfit((training$CAR_AGE+1))$lambda

ggplot(training, aes((CAR_AGE+1)^lam_car_a )) + geom_histogram(fill = 'gray', binwidth = 1, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of HOME_VAL Transformed') + theme(plot.title = element_text(hjust = 0.5))
```

BLUEBOOK

The blue book variable is moderately right skewed. We'll apply the suggested transformation by the box-cox function.

```{r}
blu_lam <- boxcoxfit(training$BLUEBOOK+1)$lambda

ggplot(training, aes((BLUEBOOK+1)^blu_lam)) + geom_histogram(fill = 'gray', binwidth = 10, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of BLUEBOOK Transformed') + theme(plot.title = element_text(hjust = 0.5))
```

OLDCLAIM

Old claim is has an extremely right skewed distribution. We'll apply a log  transformation to reduce the overall skewness.

```{r}

ggplot(training, aes(log(OLDCLAIM+1))) + geom_histogram(fill = 'gray', binwidth = 1, color = 'darkgray' ) + 
 theme_classic() + labs(title = 'Histogram of OLDCLAIM Transformed') + theme(plot.title = element_text(hjust = 0.5))
```
|   Make an alternate data set with a variet of transformations


```{r}
lm_training_transform  <-lm_training  %>%
 
   mutate(across(where(is.numeric) ,list(lam_one= ~((.+1)^boxcoxfit(.+1)$lambda) ,   lam_two=   ~BoxCox(., BoxCox.lambda(.) ) , sqrt= ~sqrt(.), log = ~log(.+1))  ) )  

evaluation_transform  <- evaluation   %>% 
 
   mutate(across(where(is.numeric) ,list(lam_one= ~((.+1)^boxcoxfit(lm_training$.+1)$lambda) ,   lam_two=   ~BoxCox(., BoxCox.lambda(lm_training$.) ) , sqrt= ~sqrt(.), log = ~log(.+1))  ) )  

```

| Test the skewness of the various "TARGET_AMT" variables. Appears that the log and Standard Box Cox Transformation have the least skewing. 

```{r}
list  = c(skewness(lm_training$TARGET_AMT),

skewness(sqrt(lm_training$TARGET_AMT)),
skewness(log(lm_training$TARGET_AMT+1)),

skewness(lm_training_transform$TARGET_AMT_lam_one),


skewness(lm_training_transform$TARGET_AMT_lam_two) )



kable(tibble( Transformation = c("Original","Square Root","Log", "Box Cox Standard","Alternate Box Cox"), Skewness =list),caption =("Skewness Values for TARGET_AMT"),format = "pdf")


```


|   Simple linear model, not using any transformations


```{r}
lm_one <- lm(TARGET_AMT~.,lm_training)


```

```{r}
summary(lm_one)
```

| Use STEPAIC from the MASS package to find the best model


```{r}
test <- stepAIC(lm_one,direction = "both",trace = F)
```




```{r}
summary(test)
```


|   We find that our residuals are highly skewed 

```{r}
plot(test)
hist(test$residuals)
skewness(test$residuals)
```






 



```{r}
lm_two <- lm(TARGET_AMT_lam_one~.-TARGET_AMT-TARGET_AMT_lam_two-TARGET_AMT_sqrt-TARGET_AMT_log,lm_training_transform)

```



```{r}
summary(lm_two)
```




```{r}
test_two <- stepAIC(lm_two,direction = "both",trace = F)

```








```{r}
summary(test_two)
```



```{r}

plot(test_two)
hist(test_two$residuals)
skewness(test_two$residuals)
```






```{r}

  eval_mod_2 <- evaluation
eval_mod_2$TARGET_AMT <-  predict.lm(test_two,  newdata=evaluation_transform)^ (1/boxcoxfit(lm_training$TARGET_AMT+1)$lambda)

hist(training$TARGET_AMT)
  
hist(eval_mod_2$TARGET_AMT)
  
summary(eval_mod_2$TARGET_AMT)
summary(lm_training$TARGET_AMT)



```



```{r}
eval_mod_2  %>% slice_max(TARGET_AMT)
```





```{r}
model_three <- lm(TARGET_AMT_log~. -TARGET_AMT -TARGET_AMT_lam_two-TARGET_AMT_lam_one -TARGET_AMT_sqrt, lm_training_transform)
```



```{r}
summary(model_three)
```


```{r}
test_three <- stepAIC(model_three,direction = "both",trace = F)
```


 



```{r}
summary(test_three)
```




```{r}
eval_mod_3 <- evaluation
eval_mod_3$TARGET_AMT <-   exp(predict.lm(test_three,  newdata=evaluation_transform) )



hist(training$TARGET_AMT)
  
hist(eval_mod_3$TARGET_AMT)
  
summary(eval_mod_3$TARGET_AMT)
summary(lm_training$TARGET_AMT)



```


 



|   Picking our model









## Please read!!!

 
|    Everything below here, not sure if I will use not. 



```{r}
set.seed(120)

log_data <- lm_training_lam_two %>% select(!ends_with(c("T_lam_one","T_lam_two","T_sqrt","T_AMT")))


train_index <- createDataPartition(log_data$TARGET_AMT_log, p=.8, list=FALSE, times = 1)
additional_train <- log_data[train_index,]
additional_test <- log_data[-train_index,]
```



```{r}
model_1 <- train(TARGET_AMT_log
                 ~., data = additional_train, 
              method = "lmStepAIC",
              trControl = trainControl(
                  method = "cv", number = 10,
                  savePredictions = TRUE),
              tuneLength = 5, 
              preProcess = c("center", "scale"))

 
```





```{r}
saveRDS(model_1,"model_one")
```


```{r}
t <- readRDS("model_one")

t$finalModel$model

```



```{r}
summary(model_1)
```



```{r}
plot(model_1$finalModel)
hist(model_1$finalModel$residuals)
skewness(model_1$finalModel$residuals)


model_1$finalMode$anova
```

```{r}
summary(model_1$finalMode)
plot(model_1$finalModel)
```





```{r}

set.seed(120)


log_alt <- lm_training %>% mutate(TARGET_AMT = log(TARGET_AMT+1))

train_index_2 <- createDataPartition(log_alt$TARGET_AMT, p=.8, list=FALSE, times = 1)
additional_train_2 <- log_alt[train_index_2,]
additional_test_2 <- log_alt[-train_index_2,]



model_2 <- train(TARGET_AMT ~., data = additional_train_2, 
              method = "lmStepAIC",
              trControl = trainControl(
                  method = "cv", number = 10,
                  savePredictions = TRUE),
              tuneLength = 5, 
              preProcess = c("center", "scale"))

```

```{r}
summary(model_2)
```


```{r}
summary(model_2$finalModel)
```

```{r}
plot(model_2$finalModel)
hist(model_2$finalModel$residuals)
skewness(model_2$finalModel$residuals)
```



```{r}
summary(lm(formula = TARGET_AMT ~ KIDSDRIV3 + INCOME + MSTATUSYes + EDUCATIONMasters + 
    EDUCATIONPhD + BLUEBOOK + CLM_FREQ4 + MVR_PTS, data = log_alt))
```


```{r}
set.seed(120)


 

train_index_3 <- createDataPartition(lm_training$TARGET_AMT, p=.8, list=FALSE, times = 1)
additional_train_3 <- lm_training[train_index_3,]
additional_test_3 <- lm_training[-train_index_3,]



model_3 <- train(TARGET_AMT ~., data = additional_train_3, 
              method = "lmStepAIC",
              trControl = trainControl(
                  method = "cv", number = 10,
                  savePredictions = TRUE),
              tuneLength = 5, 
              preProcess = c("center", "scale"))

```



```{r}
summary(model_3)
```

