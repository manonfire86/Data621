---
title: "Homework_2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(knitr)
library(caret)
library(pROC)
```

Step 1:
```{r dataload}
data <- read.csv("https://raw.githubusercontent.com/marioipena/DATA621/main/classification-output-data.csv", header = TRUE)
```


Step 2:
```{r dataframe_function}
cm <- as.data.frame(table("Actual Class" = data$class, "Predicted Class" = data$scored.class))
kable(cm)
```


Step 3:
```{r accuracy}
accuracy <- function(df){
  cm <- as.data.frame(table("Actual Class" = df$class, "Predicted Class" = df$scored.class))
  accuracy <- (cm$Freq[1] + cm$Freq[4])/sum(cm$Freq)
return(accuracy)
}
```

```{r print_accuracy}
accuracy(data)
```

Step 4:
```{r classification_error_rate}
class_error_rate <- function(df){
  cm <- as.data.frame(table("Actual Class" = df$class, "Predicted Class" = df$scored.class))
  class_error_rate <- (cm$Freq[2] + cm$Freq[3])/sum(cm$Freq)
return(class_error_rate)
}
```

```{r}
class_error_rate(data)
```

Verifying that accuracy and error rate sum up to one.
```{r verification}
accuracy_result <- accuracy(data)
error_result <- class_error_rate(data)
verify <- accuracy_result + error_result
verify
```

Step 5:
```{r precision}
precision <- function(df){
  cm <- as.data.frame(table("Actual Class" = df$class, "Predicted Class" = df$scored.class))
  precision <- cm$Freq[1]/(cm$Freq[1]+cm$Freq[3])
return(precision)
}
```

```{r print_precision}
precision(data)
```

Step 6:
```{r sensitivity}
sensitivity <- function(df){
  cm <- as.data.frame(table("Actual Class" = df$class, "Predicted Class" = df$scored.class))
  sensitivity <- cm$Freq[1]/(cm$Freq[1]+cm$Freq[2])
return(sensitivity)
}
```

```{r print_sensitivity}
sensitivity(data)
```

Step 7:
```{r specificity}
specificity <- function(df){
  cm <- as.data.frame(table("Actual Class" = df$class, "Predicted Class" = df$scored.class))
  specificity <- cm$Freq[4]/(cm$Freq[4]+cm$Freq[3])
return(specificity)
}
```

```{r print_specificity}
specificity(data)
```

Step 8:
```{r f1_score}
f1_score <- function(df){
  cm <- as.data.frame(table("Actual Class" = df$class, "Predicted Class" = df$scored.class))
  f1_score <- (2 * precision(df) * sensitivity(df)) / (precision(df) + sensitivity(df))
return(f1_score)
}
```

```{r f1_score_print}
f1_score(data)
```


Step 10:


```{r ROC_Curve}

ROC_Calc = function(x,y,...){
  TPR_ROC = c()
  FPR_ROC = c()
  for (i in seq(0,1,.01)){
    t_df = data.frame(class = x, scored.class = ifelse(y >= i,1,0),scored.probability=y)
    TPR_ROC = c(TPR_ROC,sensitivity(t_df))
    FPR_ROC = c(FPR_ROC,1 - specificity(t_df))
  }
  
  t_df = data.frame(TPR_ROC,FPR_ROC)
  t_df = t_df[complete.cases(t_df), ]
  AUC = round(sum(t_df$TPR_ROC * c(diff(t_df$FPR_ROC),0)) 
               + sum(c(diff(t_df$TPR_ROC),0) * c(diff(t_df$FPR_ROC),0))/2, 4)
  
  plot(FPR_ROC, TPR_ROC, 'l', ...)
  grid (10,10, lty = 6, col = "lightgrey")
  polygon(c(FPR_ROC, 1,1), c(TPR_ROC, 0, 1), col = 'deeppink4',density = 20, angle = 45)
  polygon(c(0,0,1,1), c(0,1,1,0), col = 'black',density = 0, lty = 6)
  abline(a=0,b=1)
  legend(0.5,0.4, AUC, title = 'AUC')
  return(paste0("AUC Score: ",AUC))
}

ROC_Calc(data$class,data$scored.probability,main = 'ROC Curve',
         xlab = 'False Positive Rate',
         ylab = 'True Positive Rate')

```

Step 12:

```{r caret_POW}
car_cm = confusionMatrix(table("Actual Class" = data$class, "Predicted Class" = data$scored.class))
```

```{r print_caretcm}
car_cm
```

```{r print_caretbyclass}
car_cm$byClass
```

Step 13:

```{r pROC_Curve}
par(mfrow= c(1,2))
plot(pROC::roc(data$class,data$scored.probability), print.auc = TRUE, main = 'ROC Curve (pROC)')
ROC_Calc(data$class,data$scored.probability,main = 'ROC Curve',
         xlab = 'False Positive Rate',
         ylab = 'True Positive Rate')
```